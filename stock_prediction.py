# -*- coding: utf-8 -*-
"""Stock Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19A9ryV46uNcHmZI-wTN7d88XUwjKUJ6A
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as pdr

"""# 1. Data Importing"""

import yfinance as yf

# Define ticker and data range
ticker='MSFT'
start='2008-12-12'
end='2025-10-4'

df=yf.download(ticker,start,end)
df.head()

"""# 2. Data Preprocessing"""

df.columns=['Close','High','Low','Open','Volume']
df.head()

df=df.reset_index()
df.head()

df=df.drop(['Date'],axis=1)
df.head()

plt.plot(df['Close'])

"""# 3. Feature Engineering"""

ma100=df.rolling(100).mean()
ma200=df.rolling(200).mean()

plt.figure(figsize=(12,6))
plt.plot(df['Close'])
plt.plot(ma100['Close'],color='r')
plt.plot(ma200['Close'],color='g')

df.shape

training_data=df['Close'][:int(len(df)*0.7)]
testing_data=df['Close'][int(len(df)*0.7):]

training_data.shape,testing_data.shape

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))

transformed_training_data=scaler.fit_transform(np.array(training_data).reshape(-1,1))
transformed_testing_data=scaler.fit_transform(np.array(testing_data).reshape(-1,1))

x_train=[]
y_train=[]
for i in range(100,transformed_training_data.shape[0]):
  x_train.append(transformed_training_data[i-100:i])
  y_train.append(transformed_training_data[i,0])

x_train,y_train=np.array(x_train),np.array(y_train)

"""# 4. Model Building"""

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,GRU,Dropout

model=Sequential([
    GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1)),
    Dropout(0.2),
    GRU(60),
    Dropout(0.3),
    Dense(1)
])

model.summary()

model.compile(optimizer='adam',loss='mean_squared_error')
history=model.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15)

model1=Sequential([
    GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1)),
    Dropout(0.2),
    GRU(60),
    Dropout(0.3),
    Dense(1)
])

from keras.callbacks import EarlyStopping
early_stopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)
model1.compile(optimizer='adam',loss='mean_squared_error')
history1=model1.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15,callbacks=[early_stopping])

model2=Sequential([
    GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1)),
    Dropout(0.2),
    GRU(60),
    Dropout(0.3),
    Dense(1)
])

from keras.callbacks import EarlyStopping,ReduceLROnPlateau
early_stopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)
reduce_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.25)
model2.compile(optimizer='adam',loss='mean_squared_error')
history2=model2.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15,callbacks=[early_stopping,reduce_lr])

plt.figure(figsize=(12,6))
plt.plot(history.history['loss'],label='Training Loss',color='b')
plt.plot(history.history['val_loss'],label='Validation Loss',color='r')

plt.plot(history1.history['loss'],label='Training Loss1',color='y')
plt.plot(history1.history['val_loss'],label='Validation Loss1',color='g')

plt.plot(history2.history['loss'],label='Training Loss2',color='violet')
plt.plot(history2.history['val_loss'],label='Validation Loss2',color='orange')
plt.legend()

"""# Experimenting with Bidirectional LSTM"""

from keras.layers import Bidirectional
model3=Sequential([
    Bidirectional(GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1))),
    Dropout(0.2),
    Bidirectional(GRU(60)),
    Dropout(0.3),
    Dense(1)
])

model3.compile(optimizer='adam',loss='mean_squared_error')
history3=model3.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15)

model4=Sequential([
    Bidirectional(GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1))),
    Dropout(0.2),
    Bidirectional(GRU(60)),
    Dropout(0.3),
    Dense(1)
])

model4.compile(optimizer='adam',loss='mean_squared_error')
history4=model4.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15,callbacks=[early_stopping,reduce_lr])

model5=Sequential([
    Bidirectional(GRU(60,return_sequences=True,input_shape=(x_train.shape[1],1))),
    Dropout(0.2),
    Bidirectional(GRU(60)),
    Dropout(0.3),
    Dense(1)
])

model5.compile(optimizer='adam',loss='mean_squared_error')
history5=model5.fit(x_train,y_train,epochs=25,batch_size=32,validation_split=0.15,callbacks=[early_stopping])

plt.figure(figsize=(12,6))
plt.plot(history3.history['loss'],label='Training Loss',color='b')
plt.plot(history3.history['val_loss'],label='Validation Loss',color='r')

plt.plot(history4.history['loss'],label='Training Loss1',color='y')
plt.plot(history4.history['val_loss'],label='Validation Loss1',color='g')

plt.plot(history5.history['loss'],label='Training Loss2',color='violet')
plt.plot(history5.history['val_loss'],label='Validation Loss2',color='orange')
plt.legend()

"""# 5. Prediction"""

last_100_days=training_data.tail(100)
final_testing_df=pd.concat([last_100_days,testing_data])

input_data=scaler.fit_transform(np.array(final_testing_df).reshape(-1,1))
input_data

x_test=[]
y_test=[]

for i in range(100,input_data.shape[0]):
  x_test.append(input_data[i-100:i])
  y_test.append(input_data[i,0])

x_test,y_test=np.array(x_test),np.array(y_test)

# making predictions
y_predicted=model.predict(x_test)
y_predicted1=model1.predict(x_test)
y_predicted2=model2.predict(x_test)

scaler.scale_

scale_factor=1/scaler.scale_[0]
y_predicted=y_predicted*scale_factor
y_predicted1=y_predicted1*scale_factor
y_predicted2=y_predicted2*scale_factor
y_test=y_test*scale_factor

plt.figure(figsize=(12,6))
plt.plot(y_test,'b',label='Original Price')
plt.plot(y_predicted,'r',label='Predicted Price1')
plt.plot(y_predicted1,'g',label='Predicted Price2')
plt.plot(y_predicted2,'y',label='Predicted Price3')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()

from sklearn.metrics import r2_score
r2_score(y_test,y_predicted),r2_score(y_test,y_predicted1),r2_score(y_test,y_predicted2)

from sklearn.metrics import mean_absolute_error,root_mean_squared_error
print('MAE1:',mean_absolute_error(y_test,y_predicted))
print('RMSE1:',root_mean_squared_error(y_test,y_predicted))
print('MAE2:',mean_absolute_error(y_test,y_predicted1))
print('RMSE2:',root_mean_squared_error(y_test,y_predicted1))
print('MAE3:',mean_absolute_error(y_test,y_predicted2))
print('RMSE3:',root_mean_squared_error(y_test,y_predicted2))

"""# for Bidirectional"""

y_predicted3=model3.predict(x_test)
y_predicted4=model4.predict(x_test)
y_predicted5=model5.predict(x_test)

scaler.scale_

scale_factor=1/scaler.scale_[0]
y_predicted3=y_predicted3*scale_factor
y_predicted4=y_predicted4*scale_factor
y_predicted5=y_predicted5*scale_factor

plt.figure(figsize=(12,6))
plt.plot(y_test,'b',label='Original Price')
plt.plot(y_predicted3,'r',label='Predicted Price4')
plt.plot(y_predicted4,'g',label='Predicted Price5')
plt.plot(y_predicted5,'y',label='Predicted Price6')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()

r2_score(y_test,y_predicted3),r2_score(y_test,y_predicted4),r2_score(y_test,y_predicted5)

print('MAE4:',mean_absolute_error(y_test,y_predicted3))
print('RMSE4:',root_mean_squared_error(y_test,y_predicted3))
print('MAE5:',mean_absolute_error(y_test,y_predicted4))
print('RMSE5:',root_mean_squared_error(y_test,y_predicted4))
print('MAE6:',mean_absolute_error(y_test,y_predicted5))
print('RMSE6:',root_mean_squared_error(y_test,y_predicted5))

"""# 6.Final Evaluation"""

from sklearn.metrics import mean_absolute_percentage_error
mean_absolute_percentage_error(y_test,y_predicted)*100,mean_absolute_percentage_error(y_test,y_predicted1)*100,mean_absolute_percentage_error(y_test,y_predicted2)*100

mean_absolute_percentage_error(y_test,y_predicted3)*100,mean_absolute_percentage_error(y_test,y_predicted4)*100,mean_absolute_percentage_error(y_test,y_predicted5)*100

model.save('stock_prediction.keras')